from tokenizer.tokenizer import get_tokens

tokens = get_tokens("grammar.txt", "input.txt", print_tokens=True)
